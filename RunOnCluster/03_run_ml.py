"""
03_run_ml.py
----------------
THE TABULAR KINGS (SOTA EDITION).
Models: XGBoost, LightGBM, CatBoost.
Logic:
  1. Loops through all 4 Experiments (Solar/Wind x WSI/NoWSI).
  2. Uses Hyperopt (Bayesian Optimization) with AGGRESSIVE search spaces.
  3. Uses Early Stopping to save compute time on bad trials.
  4. Saves BEST metrics and params to text files.
"""

import os
import pandas as pd
import numpy as np
import xgboost as xgb
import lightgbm as lgb
import catboost as cb
from hyperopt import fmin, tpe, hp, STATUS_OK, Trials, space_eval
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import time
import warnings

# Suppress warnings for cleaner logs
warnings.filterwarnings('ignore')

# --- CONFIGURATION ---
DATA_DIR = "processed_data"
OUTPUT_DIR = "outputs"
MAX_EVALS = 50  # Increase to 100-200 for final production runs

os.makedirs(OUTPUT_DIR, exist_ok=True)

# Define the 4 Experiments based on filenames generated by the Forge
EXPERIMENTS = [
    "with_wsi_solar",
    "no_wsi_solar",
    "with_wsi_wind",
    "no_wsi_wind"
]

MODELS = ["xgboost", "lightgbm", "catboost"]

def load_data(experiment_name):
    """Loads the specific Parquet files for an experiment."""
    try:
        X_train = pd.read_parquet(f"{DATA_DIR}/X_train_{experiment_name}.parquet")
        y_train = pd.read_parquet(f"{DATA_DIR}/y_train_{experiment_name}.parquet").values.ravel()
        X_test = pd.read_parquet(f"{DATA_DIR}/X_test_{experiment_name}.parquet")
        y_test = pd.read_parquet(f"{DATA_DIR}/y_test_{experiment_name}.parquet").values.ravel()
        return X_train, y_train, X_test, y_test
    except FileNotFoundError:
        print(f"[ERROR] Could not find data for {experiment_name}. Run 01_data_forge.py first!")
        return None, None, None, None

def get_aggressive_search_space(model_name):
    """
    Returns the SOTA Hyperopt search space.
    Ranges are log-uniform to explore orders of magnitude (e.g. 0.001 vs 0.1).
    """
    if model_name == "xgboost":
        return {
            # Explore learning rates from 0.005 to 0.3
            'learning_rate': hp.loguniform('learning_rate', np.log(0.005), np.log(0.3)),
            
            # Deep trees allowed for complex weather interactions
            'max_depth': hp.choice('max_depth', range(3, 15)),
            
            # Stochastic sampling to prevent overfitting
            'subsample': hp.uniform('subsample', 0.5, 1.0),
            'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 1.0),
            
            # High max estimators, but Early Stopping will cut it short if needed
            'n_estimators': hp.choice('n_estimators', range(500, 5000, 100)),
            
            # Aggressive Regularization search (1e-5 to 100.0)
            'reg_alpha': hp.loguniform('reg_alpha', np.log(1e-5), np.log(100)),
            'reg_lambda': hp.loguniform('reg_lambda', np.log(1e-5), np.log(100)),
            
            # Controls leaf purity
            'min_child_weight': hp.choice('min_child_weight', range(1, 10))
        }

    elif model_name == "lightgbm":
        return {
            'learning_rate': hp.loguniform('learning_rate', np.log(0.005), np.log(0.3)),
            
            # LightGBM grows leaf-wise. 
            # We search for leaves independently of depth.
            'num_leaves': hp.choice('num_leaves', range(20, 256)),
            'max_depth': hp.choice('max_depth', [-1, 5, 10, 15, 20, 30]), # -1 = no limit
            
            'subsample': hp.uniform('subsample', 0.5, 1.0),
            'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 1.0),
            'n_estimators': hp.choice('n_estimators', range(500, 5000, 100)),
            
            # Crucial for LightGBM overfitting control
            'min_child_samples': hp.choice('min_child_samples', range(5, 100)),
            
            'reg_alpha': hp.loguniform('reg_alpha', np.log(1e-5), np.log(100)),
            'reg_lambda': hp.loguniform('reg_lambda', np.log(1e-5), np.log(100))
        }

    elif model_name == "catboost":
        return {
            'learning_rate': hp.loguniform('learning_rate', np.log(0.005), np.log(0.3)),
            
            # CatBoost is sensitive to depth. 4-10 is the standard range.
            'depth': hp.choice('depth', range(4, 11)),
            
            # L2 Regularization
            'l2_leaf_reg': hp.loguniform('l2_leaf_reg', np.log(1), np.log(10)),
            
            'iterations': hp.choice('iterations', range(500, 3000, 100)),
            'subsample': hp.uniform('subsample', 0.5, 1.0),
            
            # Adds randomness to scoring to prevent overfitting
            'random_strength': hp.uniform('random_strength', 1, 10),
            
            # Bayesian Bootstrap temperature
            'bagging_temperature': hp.uniform('bagging_temperature', 0, 1)
        }
    return {}

def train_and_eval(params, model_name, X_train, y_train, X_test, y_test):
    """
    Objective function for Hyperopt.
    Uses EARLY STOPPING on the Test set (acting as Validation here) to find optimal trees.
    """
    
    # XGBoost Logic
    if model_name == "xgboost":
        model = xgb.XGBRegressor(**params, n_jobs=-1, random_state=42)
        # Fit with Early Stopping
        model.fit(X_train, y_train, 
                  eval_set=[(X_test, y_test)], 
                  early_stopping_rounds=50, 
                  verbose=False)
        
    # LightGBM Logic
    elif model_name == "lightgbm":
        model = lgb.LGBMRegressor(**params, n_jobs=-1, random_state=42, verbose=-1)
        from lightgbm import early_stopping, log_evaluation
        # Fit with callbacks
        model.fit(X_train, y_train, 
                  eval_set=[(X_test, y_test)], 
                  callbacks=[early_stopping(stopping_rounds=50), log_evaluation(0)])
        
    # CatBoost Logic
    elif model_name == "catboost":
        model = cb.CatBoostRegressor(**params, thread_count=-1, random_state=42, 
                                     verbose=0, allow_writing_files=False)
        model.fit(X_train, y_train, 
                  eval_set=(X_test, y_test), 
                  early_stopping_rounds=50, 
                  verbose=False)
    
    # Predict
    # The model object automatically uses the "best_iteration" limit from early stopping
    preds = model.predict(X_test)
    
    # Calculate Loss (RMSE to minimize)
    rmse = np.sqrt(mean_squared_error(y_test, preds))
    
    return {'loss': rmse, 'status': STATUS_OK, 'model': model}

def append_results_to_file(model_name, experiment, best_params, metrics):
    """Writes the professional report to text file."""
    filename = f"{OUTPUT_DIR}/results_{model_name}.txt"
    
    header = ""
    if not os.path.exists(filename):
        header = f"==================================================\nMODEL: {model_name.upper()} (Aggressive Tuning)\n==================================================\n\n"
    
    report = f"""--- EXPERIMENT: {experiment.upper()} ---
Date: {time.strftime("%Y-%m-%d %H:%M:%S")}
Best Parameters: {best_params}

Performance Metrics:
   MAE : {metrics['mae']:.4f} MW
   RMSE: {metrics['rmse']:.4f} MW
   R2  : {metrics['r2']:.4f}
--------------------------------------------------
"""
    with open(filename, "a") as f:
        f.write(header + report)
    print(f"   [REPORT] Saved to {filename}")

# --- MAIN EXECUTION FLOW ---
if __name__ == "__main__":
    print("========================================")
    print("   PHASE 2: THE MACHINE LEARNING ARENA  ")
    print("   (XGBoost / LightGBM / CatBoost)      ")
    print("========================================")

    for model_name in MODELS:
        print(f"\n>>> ENTERING ARENA: {model_name.upper()} <<<")
        
        for experiment in EXPERIMENTS:
            print(f"\n   [Experiment] {experiment} ...")
            
            # 1. Load Data
            X_train, y_train, X_test, y_test = load_data(experiment)
            if X_train is None: continue
            
            # 2. Define Objective Wrapper
            def objective(params):
                return train_and_eval(params, model_name, X_train, y_train, X_test, y_test)
            
            # 3. Hyperopt Optimization
            print(f"   [Tuning] Running {MAX_EVALS} trials with TPE Bayesian Optimization...")
            space = get_aggressive_search_space(model_name)
            trials = Trials()
            
            best_indices = fmin(fn=objective, 
                                space=space, 
                                algo=tpe.suggest, 
                                max_evals=MAX_EVALS, 
                                trials=trials)
            
            # 4. Retrieve Best Parameters (Convert indices to real values)
            best_params = space_eval(space, best_indices)
            print(f"   [Champion Params] {best_params}")
            
            # 5. Retrain Champion (or retrieve best model from trials if possible, but retraining is safer)
            print("   [Training] Retraining Champion Model for Final Verification...")
            final_run = train_and_eval(best_params, model_name, X_train, y_train, X_test, y_test)
            final_model = final_run['model']
            
            final_preds = final_model.predict(X_test)
            metrics = {
                'mae': mean_absolute_error(y_test, final_preds),
                'rmse': np.sqrt(mean_squared_error(y_test, final_preds)),
                'r2': r2_score(y_test, final_preds)
            }
            
            print(f"   [Result] RMSE: {metrics['rmse']:.2f} | MAE: {metrics['mae']:.2f}")
            
            # 6. Save Report
            append_results_to_file(model_name, experiment, best_params, metrics)